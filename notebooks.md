# Zero‚Äëto‚ÄëHero –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Äî —Ä–∞–±–æ—á–∞—è —Ç–µ—Ç—Ä–∞–¥–∫–∞

> –£—á–µ–±–Ω–∞—è —Ç–µ—Ç—Ä–∞–¥–∫–∞ –ø–æ –º–æ—Ç–∏–≤–∞–º –∫—É—Ä—Å–∞ **Andrej Karpathy ‚Äî NN: Zero‚Äëto‚ÄëHero**.  
> –ó–¥–µ—Å—å ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã + –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ
> –º–∏–Ω–∏‚Äë—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏: *–º–∏–∫—Ä–æ‚Äëautograd*, *char‚ÄëLM (makemore)*, *–≤–Ω–∏–º–∞–Ω–∏–µ* –∏ *–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π Transformer‚Äë–±–ª–æ–∫*.
>
> **–§–æ—Ä–º–∞—Ç:** –æ–±—ã—á–Ω—ã–π `Markdown`. –ï–≥–æ –º–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å –Ω–∞ GitHub, –∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∫–æ–¥–∞
> ‚Äì –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –±–ª–æ–∫–∏ –≤ Python/Colab/Jupyter. –ü—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤
> –Ω–æ—É—Ç–±—É–∫: `jupytext --to ipynb notebooks.md`.

---

## 0. –ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è

1. –ò–¥—ë—Ç–µ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º —Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑.  
2. –í –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ –µ—Å—Ç—å **–∫–æ–¥**, **–∑–∞–¥–∞–Ω–∏—è (‚úÖ)** –∏ **–≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ (‚ùì)**.  
3. –í—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç **–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö** ‚Äî –º—ã –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–≥—Ä—É—à–µ—á–Ω—ã–µ –∫–æ—Ä–ø—É—Å–∞.

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è Python:** 3.10+  
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –¥–ª—è PyTorch‚Äë—á–∞—Å—Ç–∏ (—Ä–∞–∑–¥–µ–ª—ã 3‚Äì4) —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `torch`.

```bash
pip install torch jupytext  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
```

---

## 1. –ú–∏–∫—Ä–æ‚Äëautograd: –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–µ–∞–ª–∏–∑—É–µ–º **—Å–∫–∞–ª—è—Ä–Ω—É—é** –∞–≤—Ç–æ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏—é –≤ –¥—É—Ö–µ *micrograd*.
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –∏ PyTorch –ø–æ–¥–∫–ª—é—á–∏–º –ø–æ–∑–∂–µ.

### 1.1. –£–∑–ª—ã –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞

```python
import math

class Value:
    def __init__(self, data, _prev=(), _op='', label=''):
        self.data = float(data)
        self.grad = 0.0
        self._prev = set(_prev)
        self._op = _op
        self.label = label
        self._backward = lambda: None

    def __repr__(self):
        return f"Value(data={self.data:.4f}, grad={self.grad:.4f})"

    # --- –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ ---
    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += 1.0 * out.grad
            other.grad += 1.0 * out.grad
        out._backward = _backward
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data  * out.grad
        out._backward = _backward
        return out

    def __neg__(self):        # -a
        return self * -1
    def __sub__(self, other): # a - b
        return self + (-other)
    def __truediv__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return self * other**-1
    def __pow__(self, p):
        assert isinstance(p, (int,float))
        out = Value(self.data**p, (self,), f'**{p}')
        def _backward():
            self.grad += (p * (self.data**(p-1))) * out.grad
        out._backward = _backward
        return out
    def tanh(self):
        t = (math.exp(2*self.data) - 1) / (math.exp(2*self.data) + 1)
        out = Value(t, (self,), 'tanh')
        def _backward():
            self.grad += (1 - t**2) * out.grad
        out._backward = _backward
        return out

def topological_sort(root):
    """–û–±—Ö–æ–¥ –≥—Ä–∞—Ñ–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ backprop"""
    seen, order = set(), []
    def build(v):
        if v not in seen:
            seen.add(v)
            for child in v._prev: build(child)
            order.append(v)
    build(root)
    return order

def backward(loss):
    for v in topological_sort(loss)[::-1]:
        v._backward()
```
**–ü—Ä–æ–≤–µ—Ä–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã** –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–∏:

```python
# f = (a*b + c)^2
a, b, c = Value(2.0), Value(-3.0), Value(1.5)
f = (a*b + c)**2
# –ø—Ä—è–º–æ–π –ø—Ä–æ–≥–æ–Ω
f.grad = 1.0
backward(f)
print(a, b, c, f)
```

### ‚úÖ –ó–∞–¥–∞–Ω–∏—è
1. –î–æ–±–∞–≤—å—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ `exp`, `relu` –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ –≤—ã—Ä–∞–∂–µ–Ω–∏–∏ –≤–∞—à–µ–≥–æ –≤—ã–±–æ—Ä–∞.
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π `Linear1(x) = w*x + b` –¥–ª—è —Å–∫–∞–ª—è—Ä–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–µ–π—Ä–æ–Ω—É).

### ‚ùì –°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞
- –ß–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏–∏?
- –ü–æ—á–µ–º—É –Ω—É–∂–µ–Ω **—Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫** –ø—Ä–∏ backprop?

---

## 2. –ú–∏–Ω–∏‚ÄëMLP –Ω–∞ –Ω–∞—à–µ–º autograd

–°–æ–±–µ—Ä—ë–º –∫—Ä–æ—à–µ—á–Ω—É—é —Å–µ—Ç—å –∏ –æ–±—É—á–∏–º –µ—ë –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é.

```python
import random

random.seed(0)

class Neuron:
    def __init__(self, nin):
        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
        self.b = Value(0.0)

    def __call__(self, x):  # x: list[float] –∏–ª–∏ list[Value]
        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)
        out = act.tanh()
        return out

    def parameters(self):
        return self.w + [self.b]

class MLP:
    def __init__(self, nin, nout, nh=16):
        self.h = [Neuron(nin) for _ in range(nh)]
        self.out = [Neuron(nh) for _ in range(nout)]

    def __call__(self, x):
        h = [n(x) for n in self.h]
        y = [n(h) for n in self.out]
        return y

    def parameters(self):
        ps = []
        for n in self.h + self.out:
            ps.extend(n.parameters())
        return ps

# –ò–≥—Ä—É—à–µ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: y = sin(x0) + cos(x1)
def make_batch(bs=32):
    import math, random
    X, Y = [], []
    for _ in range(bs):
        x0, x1 = random.uniform(-3,3), random.uniform(-3,3)
        y = math.sin(x0) + math.cos(x1)
        X.append([Value(x0), Value(x1)])
        Y.append(Value(y))
    return X, Y

net = MLP(nin=2, nout=1, nh=16)

def mse(yhat, y):
    return sum(( (yh - yt)**2 for yh,yt in zip(yhat, y) ), Value(0.0)) / len(y)

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
for step in range(200):
    X, Y = make_batch(32)
    Yhat = [net(x)[0] for x in X]
    loss = mse(Yhat, Y)

    # –æ–±–Ω—É–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    for p in net.parameters(): p.grad = 0.0
    loss.grad = 1.0
    backward(loss)

    # —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    for p in net.parameters():
        p.data += -0.05 * p.grad

    if step % 50 == 0:
        print(f"step {step:03d} loss={loss.data:.4f}")
```

### ‚úÖ –ó–∞–¥–∞–Ω–∏—è
1. –î–æ–±–∞–≤—å—Ç–µ L2‚Äë—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é.  
2. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ç–∞—Ä–≥–µ—Ç‚Äë—Ñ—É–Ω–∫—Ü–∏–∏: `tanh` vs `relu`. –ì–¥–µ —Å—Ö–æ–¥–∏—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ –∏ –ø–æ—á–µ–º—É?

---

## 3. Char‚ÄëLM (*makemore*): n‚Äë–≥—Ä–∞–º–º—ã ‚Üí MLP ‚Üí —Å—ç–º–ø–ª–∏–Ω–≥

–¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–π–¥—ë–º –∫ **—Å–∏–º–≤–æ–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏**. –ù–∞—á–Ω—ë–º —Å n‚Äë–≥—Ä–∞–º–º –∏ –ø—Ä–æ—Å—Ç–æ–π MLP.

### 3.1. –î–∞—Ç–∞—Å–µ—Ç (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º–µ–Ω–∞)

```python
import random, string

random.seed(1)
NAMES = ["anna","maria","pavel","boris","lena","olga","roman","dima","ivan","nina","lola","max","katya","petya","sara"]
# —Å–ø–µ—Ü. —Å–∏–º–≤–æ–ª—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞
BOS, EOS = "<", ">"

def build_vocab(names):
    chars = sorted(set("".join(names)))
    itos = {i+1:c for i,c in enumerate(chars)}  # 0 –æ—Å—Ç–∞–≤–∏–º –ø–æ–¥ BOS
    itos[0] = BOS
    itos[len(itos)] = EOS
    stoi = {c:i for i,c in itos.items()}
    return stoi, itos

stoi, itos = build_vocab(NAMES)
vocab_size = len(itos)
vocab_size
```

### 3.2. –û–±—É—á–∞—é—â–∏–µ –ø–∞—Ä—ã –¥–ª—è –±–∏–≥—Ä–∞–º–º

```python
def make_bigrams(names):
    pairs = []
    for name in names:
        s = BOS + name + EOS
        for ch_prev, ch_next in zip(s, s[1:]):
            pairs.append((stoi[ch_prev], stoi[ch_next]))
    return pairs

pairs = make_bigrams(NAMES)

# –ú–∞—Ç—Ä–∏—Ü–∞ –ø–æ–¥—Å—á—ë—Ç–æ–≤ (MLE)
import numpy as np
C = np.ones((vocab_size, vocab_size), dtype=np.float32)  # +1 ‚Äî —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
for i,j in pairs: C[i,j] += 1.0
P = C / C.sum(axis=1, keepdims=True)  # —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞
```

### 3.3. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º—ë–Ω –ø–æ –±–∏–≥—Ä–∞–º–º–Ω–æ–π –º–æ–¥–µ–ª–∏

```python
import numpy as np

def sample_bigram(n=10):
    out = []
    for _ in range(n):
        s = BOS
        while True:
            i = stoi[s[-1]]
            j = np.random.choice(vocab_size, p=P[i])
            ch = itos[j]
            if ch == EOS: break
            s += ch
        out.append(s[1:])  # —É–±—Ä–∞—Ç—å BOS
    return out

print(sample_bigram(10))
```

### 3.4. MLP‚Äë–º–æ–¥–µ–ª—å –¥–ª—è (–∫–æ–Ω—Ç–µ–∫—Å—Ç‚Üí—Å–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª)

```python
import torch, torch.nn as nn

K = 3  # –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
# –ø–æ—Å—Ç—Ä–æ–∏–º –¥–∞—Ç–∞—Å–µ—Ç (X: [N,K], Y: [N])
def build_dataset(names, K):
    X, Y = [], []
    for name in names:
        s = BOS + name + EOS
        ctx = [0]*K
        for ch in s:
            ix = stoi[ch]
            X.append(ctx[:])
            Y.append(ix)
            ctx = ctx[1:] + [ix]
    return torch.tensor(X), torch.tensor(Y)

Xtr, Ytr = build_dataset(NAMES, K)

model = nn.Sequential(
    nn.Embedding(vocab_size, 16),
    nn.Flatten(),
    nn.Linear(16*K, 64), nn.Tanh(),
    nn.Linear(64, vocab_size)
)

opt = torch.optim.AdamW(model.parameters(), lr=1e-2)
lossf = nn.CrossEntropyLoss()

for step in range(2000):
    logits = model(Xtr)
    loss = lossf(logits, Ytr)
    opt.zero_grad(); loss.backward(); opt.step()
    if step % 400 == 0:
        print(step, loss.item())
```

**–°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å MLP:**

```python
import torch
def sample_mlp(n=10):
    res = []
    for _ in range(n):
        ctx = [0]*K
        out = ""
        while True:
            X = torch.tensor([ctx])
            logits = model(X)
            ix = torch.distributions.Categorical(logits=logits).sample().item()
            ch = itos[ix]
            if ch == EOS: break
            out += ch
            ctx = ctx[1:] + [ix]
        res.append(out)
    return res

print(sample_mlp(10))
```

### ‚úÖ –ó–∞–¥–∞–Ω–∏—è
1. –ü–æ–∏–≥—Ä–∞–π—Ç–µ —Å `K` –∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å–ª–æ—ë–≤; –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏/–≤–∞–ª–∏–¥–∞—Ü–∏–∏.  
2. –î–æ–±–∞–≤—å—Ç–µ **dropout** –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ.  
3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ *temperature sampling* –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.  

---

## 4. –í–Ω–∏–º–∞–Ω–∏–µ (Scaled Dot‚ÄëProduct) ‚Äî –∏–≥—Ä—É—à–µ—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä

–°–¥–µ–ª–∞–µ–º –æ–¥–∏–Ω —Å–ª–æ–π **—Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è** (self‚Äëattention) –ø–æ–≤–µ—Ä—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

```python
import torch, torch.nn as nn, math

torch.manual_seed(42)

B, T, C = 2, 5, 8  # batch, time, channels
x = torch.randn(B, T, C)

d_k = C
Wq = nn.Linear(C, C, bias=False)
Wk = nn.Linear(C, C, bias=False)
Wv = nn.Linear(C, C, bias=False)

Q = Wq(x)  # (B,T,C)
K = Wk(x)  # (B,T,C)
V = Wv(x)  # (B,T,C)

att = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,T,T)
mask = torch.tril(torch.ones(T, T)) == 1
att = att.masked_fill(~mask, float('-inf'))  # –∫–∞—É–∑–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞
w = torch.softmax(att, dim=-1)               # –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
y = w @ V                                     # (B,T,C)
y.shape
```

### ‚ùì –í–æ–ø—Ä–æ—Å—ã
- –ó–∞—á–µ–º –¥–µ–ª–∏—Ç—å –Ω–∞ `sqrt(d_k)`?  
- –ß–µ–º **self‚Äëattention** –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ¬´–∑–∞–ø—Ä–æ—Å‚Üí–ø–∞–º—è—Ç—å¬ª?

---

## 5. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π Transformer‚Äë–±–ª–æ–∫

–°–æ–±–µ—Ä—ë–º **–∫–∞—É–∑–∞–ª—å–Ω—ã–π** –æ–¥–Ω–æ—Å–ª–æ–π–Ω—ã–π –±–ª–æ–∫: *Multi‚ÄëHead Attention ‚Üí FFN ‚Üí LayerNorm + residuals*.

```python
class Head(nn.Module):
    def __init__(self, C, head_size, T):
        super().__init__()
        self.key = nn.Linear(C, head_size, bias=False)
        self.query = nn.Linear(C, head_size, bias=False)
        self.value = nn.Linear(C, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(T, T)))
        self.scale = head_size ** -0.5
    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x); q = self.query(x); v = self.value(x)
        att = q @ k.transpose(-2,-1) * self.scale
        att = att.masked_fill(self.tril[:T,:T]==0, float('-inf'))
        w = att.softmax(dim=-1)
        return w @ v

class MultiHead(nn.Module):
    def __init__(self, C, num_heads, head_size, T):
        super().__init__()
        self.heads = nn.ModuleList([Head(C, head_size, T) for _ in range(num_heads)])
        self.proj  = nn.Linear(num_heads*head_size, C)
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        return self.proj(out)

class Block(nn.Module):
    def __init__(self, C, num_heads, T, ff=4):
        super().__init__()
        head_size = C // num_heads
        self.sa = MultiHead(C, num_heads, head_size, T)
        self.ln1 = nn.LayerNorm(C)
        self.ffn = nn.Sequential(nn.Linear(C, ff*C), nn.ReLU(), nn.Linear(ff*C, C))
        self.ln2 = nn.LayerNorm(C)
    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x
```

**–ü—Ä–∏–º–µ–Ω–∏–º –±–ª–æ–∫ –∫ char‚ÄëLM:**

```python
class TinyTransformer(nn.Module):
    def __init__(self, vocab, C=64, T=16, H=4):
        super().__init__()
        self.T = T
        self.token = nn.Embedding(vocab, C)
        self.pos   = nn.Embedding(T, C)
        self.block = Block(C=C, num_heads=H, T=T)
        self.lm    = nn.Linear(C, vocab)
    def forward(self, idx):
        B, T = idx.shape
        x = self.token(idx) + self.pos(torch.arange(T))
        x = self.block(x)
        return self.lm(x)  # (B,T,vocab)

# –∏–≥—Ä—É—à–µ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è LM
def batch_lm(names, T=16, bs=32):
    alltext = BOS + (EOS+BOS).join(names) + EOS
    ids = torch.tensor([stoi[c] for c in alltext], dtype=torch.long)
    X, Y = [], []
    for i in range(len(ids)-T):
        X.append(ids[i:i+T])
        Y.append(ids[i+1:i+T+1])
    X = torch.stack(X); Y = torch.stack(Y)
    ix = torch.randint(0, len(X), (bs,))
    return X[ix], Y[ix]

torch.manual_seed(0)
model = TinyTransformer(vocab_size, C=64, T=16, H=4)
opt = torch.optim.AdamW(model.parameters(), lr=3e-3)
lossf = nn.CrossEntropyLoss()

for step in range(800):
    X,Y = batch_lm(NAMES, T=16, bs=64)
    logits = model(X)
    loss = lossf(logits.view(-1, vocab_size), Y.view(-1))
    opt.zero_grad(); loss.backward(); opt.step()
    if step % 200 == 0:
        print(step, loss.item())
```

### ‚úÖ –ó–∞–¥–∞–Ω–∏—è
1. –°–¥–µ–ª–∞–π—Ç–µ **–º–Ω–æ–≥–æ –≥–æ–ª–æ–≤** –∏ –∏–∑–º–µ—Ä—å—Ç–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ª–æ—Å—Å.  
2. –î–æ–±–∞–≤—å—Ç–µ **dropout** –≤ MHA –∏ FFN.  
3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ *weight tying* (`lm.weight = token.weight`).

---

## 6. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ú–æ–¥–µ–ª—å | –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | –í–∞–ª. –ª–æ—Å—Å* |
|---|---:|---:|---:|
| –ë–∏–≥—Ä–∞–º–º—ã (MLE) | 1 | ~`V^2` | ‚Äî |
| MLP (K=3) | 3 | ~50k | ‚Ä¶ |
| TinyTransformer | 16 | ~110k | ‚Ä¶ |

\* –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ—é –≤–∞–ª–∏–¥–∞—Ü–∏—é (—Ä–∞–Ω–¥–æ–º–Ω—ã–π —Å–ø–ª–∏—Ç –∏–º–µ–Ω 80/20).

---

## 7. –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è (–∫–æ—Ä–æ—Ç–∫–∏–π –ª–∏—Å—Ç)

- –ß–µ–º —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç seq2seq?  
- –ü–æ—á–µ–º—É **–∫–∞—É–∑–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞** –≤–∞–∂–Ω–∞ –¥–ª—è autoregressive‚ÄëLM?  
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω `LayerNorm` –∏ residual‚Äë—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è?  
- –ß—Ç–æ —Ç–∞–∫–æ–µ *exposure bias* –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏?

---

## 8. –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω–æ—É—Ç–±—É–∫ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
pip install jupytext
jupytext --to ipynb notebooks.md

# –ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ
python - <<'PY'
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –æ–∫")
PY
```

---

## 9. –û—Ç–∫—É–¥–∞ –∏–¥–µ–∏

- –ö—É—Ä—Å **NN: Zero‚Äëto‚ÄëHero** (Andrej Karpathy) ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø—Ä–æ–π—Ç–∏ —Ü–µ–ª–∏–∫–æ–º.  
- –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —ç—Ç–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ –Ω–∞–ø–∏—Å–∞–Ω—ã ¬´—Å –Ω—É–ª—è¬ª –¥–ª—è —É—á–µ–±–Ω—ã—Ö —Ü–µ–ª–µ–π –∏
  –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç –∫–æ–¥ –∫—É—Ä—Å–∞ –¥–æ—Å–ª–æ–≤–Ω–æ.

–£–¥–∞—á–∏! üöÄ
